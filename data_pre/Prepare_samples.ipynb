{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f823f82",
   "metadata": {},
   "source": [
    "## Prepare samples using GEE\n",
    "\n",
    "### Setup\n",
    "For each site, extract the full time series of Sentinel-1,NDVI data from the GEE. Note: if the output csv files already exist they are assumed to be correct and are not over-written.\n",
    "\n",
    "Note: Proxy was set for the well known reason in China and you may not need it. Also check the proxy in the utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4bfa2765",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import ee\n",
    "os.environ['HTTP_PROXY'] = 'http://127.0.0.1:7897' # Setup the proxy if required\n",
    "os.environ['HTTPS_PROXY'] = 'http://127.0.0.1:7897'\n",
    "ee.Authenticate() # authenticate the gee account\n",
    "import utils_data_pre"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6892e463",
   "metadata": {},
   "source": [
    "Set the parameters, paths etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93a48e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "HOME_DATA_DIR = r'E:\\Demo\\data_pre' # change the dir\n",
    "# Date range for Sentinel-1 data\n",
    "START_DATE = \"2015-01-01\"\n",
    "END_DATE = \"2022-12-31\"  #\n",
    "# Date range for NDVI and weather data,one year preceding the Sentinel-1\n",
    "START_DATE_NDVI = \"2014-01-01\" # \n",
    "END_DATE_NDVI = \"2023-01-31\" #\n",
    "# Global setups, dir, path\n",
    "save_to_disk = False # No temporal files\n",
    "SM_SITES = os.path.join(HOME_DATA_DIR, \"stations.csv\") # the site informaiton extracted by Preprocessing_ISMN_Raw_Data.ipynb\n",
    "dir_to_site_sm = os.path.join(HOME_DATA_DIR, \"station_sm\") # the path to the soil moisture of stations\n",
    "dir_to_site_samples = os.path.join(HOME_DATA_DIR, \"output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e2f63e",
   "metadata": {},
   "source": [
    "Read the sites information and determine the grid size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7f2da93",
   "metadata": {},
   "outputs": [],
   "source": [
    "sites = pd.read_csv(SM_SITES, float_precision=\"high\")\n",
    "grid_size = 0.05 # km\n",
    "pobj=utils_data_pre.grids_4_a_region(4326,grid_size) # determine the grid size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9addc9",
   "metadata": {},
   "source": [
    "## A loop to prepare the input data of each site\n",
    "\n",
    "###### 1 Create the gird polygon covering a site in both EASE2.0 and WGS84\n",
    "###### 2 Extract Sentienl-1, soil texture, terrain, NDVI, precipition, temperature etc. Check the utils for the details\n",
    "###### 3 Concatenate all data\n",
    "###### 4 Extract the ground soil moisture of the site\n",
    "###### Note: the loop may report the error \"IncompleteRead\", just run this cell again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ef9d299",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(sites)):#len(sites)\n",
    "    site = sites.loc[i]\n",
    "    path_2_site_file = os.path.join(dir_to_site_samples,'%s.csv'%site.Site)\n",
    "    if os.path.exists(path_2_site_file):\n",
    "        continue\n",
    "    ring_wgs,grid_ring=pobj.get_wgs_grid(site.lon,site.lat)\n",
    "    polygon_grid=ee.Geometry.Polygon(ring_wgs, 'EPSG:4326', True, 20, False)\n",
    "    samples,df_S1=utils_data_pre.samples_4_grid_v1(polygon_grid,START_DATE, END_DATE,START_DATE_NDVI,END_DATE_NDVI,ring_wgs,pobj)\n",
    "    \n",
    "    # include the ground truth of soil moisture\n",
    "    station_sm=pd.read_csv(os.path.join(dir_to_site_sm,'%s.csv'%site.Site),parse_dates=['date'])\n",
    "    sm_point=station_sm[station_sm.date.dt.date.isin(list(df_S1.date.dt.date))]['SM']/100\n",
    "    df_S1.loc[df_S1.date.dt.date.isin(list(station_sm.date.dt.date)),'sm_25']=list(sm_point)\n",
    "    \n",
    "    samples=pd.DataFrame(samples,index=df_S1.index)\n",
    "    samples=pd.concat([df_S1,samples],axis=1)\n",
    "    samples.to_csv(path_2_site_file)\n",
    "    time.sleep(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
